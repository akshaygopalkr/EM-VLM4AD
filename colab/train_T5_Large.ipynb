{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaygopalkr/EM-VLM4AD/blob/main/train_T5_Large.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzDH8QURvRbu"
      },
      "source": [
        "## Library Importing/Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62VNPLM9sj0V"
      },
      "outputs": [],
      "source": [
        "!pip install peft\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBxF3cr7tVqZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDDN66XDsqki"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision.io import read_image\n",
        "from transformers import T5Tokenizer\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer\n",
        "from transformers import BitsAndBytesConfig\n",
        "from torchvision.models import vit_b_32\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import argparse\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, LoftQConfig\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX6bVoHhvcFE"
      },
      "source": [
        "## Model Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdKibhc1uAzO"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "VIT_HIDDEN_STATE = 768\n",
        "VIT_SEQ_LENGTH = 49\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "\n",
        "    print(\n",
        "        f\"Trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "class DriveVLMT5(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained('google-t5/t5-large', device_map='auto')\n",
        "\n",
        "        hidden_size = self.model.config.d_model\n",
        "\n",
        "\n",
        "        # Only if we are perfoming LoRA finetuning on T5\n",
        "        if config.lora:\n",
        "\n",
        "            # For quantization\n",
        "            loftq_config = LoftQConfig(loftq_bits=8) if config.quantize else None\n",
        "\n",
        "            # Create LoRA model\n",
        "            lora_config = LoraConfig(\n",
        "                r=config.lora_dim,\n",
        "                lora_alpha=config.lora_alpha,\n",
        "                loftq_config=loftq_config,\n",
        "                lora_dropout=config.lora_dropout,\n",
        "                bias='none',\n",
        "                target_modules=['q', 'v']\n",
        "            )\n",
        "            self.model = get_peft_model(self.model, lora_config)\n",
        "\n",
        "        # Freeze model weights if needed\n",
        "        if config.freeze_lm:\n",
        "\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        print('Trainable Parameters for T5 model:')\n",
        "        print_trainable_parameters(self.model)\n",
        "\n",
        "        # Create instance for multi-view processor\n",
        "        self.mvp = self.MultiViewProcessor(config.gpa_hidden_size, hidden_size, freeze=True)\n",
        "\n",
        "    class MultiViewProcessor(nn.Module):\n",
        "\n",
        "        def __init__(self, gpa_hidden_size, hidden_size, freeze=False):\n",
        "\n",
        "            super().__init__()\n",
        "\n",
        "            # Use ViT for image embeddings\n",
        "            self.img_model = vit_b_32(weights='DEFAULT')\n",
        "\n",
        "            # Modal embedding to distinguish between image and text\n",
        "            self.modal_embeddings = nn.Embedding(2, hidden_size)\n",
        "            self.modal_embeddings.weight.data.normal_(mean=0.0, std=0.02)\n",
        "\n",
        "            # If we are freezing the CLIP embeddings\n",
        "            if freeze:\n",
        "                for param in self.img_model.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "            # Set matrices based on MIVC paper\n",
        "            self.w = nn.Linear(in_features=gpa_hidden_size, out_features=1)\n",
        "            self.Z = nn.Sequential(\n",
        "                nn.Linear(in_features=VIT_HIDDEN_STATE * VIT_SEQ_LENGTH, out_features=gpa_hidden_size, bias=False),\n",
        "                nn.Tanh()\n",
        "            )\n",
        "            self.G = nn.Sequential(\n",
        "                nn.Linear(in_features=VIT_HIDDEN_STATE * VIT_SEQ_LENGTH, out_features=gpa_hidden_size, bias=False),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "            self.img_projection_layer = nn.Linear(in_features=VIT_HIDDEN_STATE, out_features=hidden_size)\n",
        "\n",
        "\n",
        "        def gpa(self, img_embeddings):\n",
        "\n",
        "            \"\"\"\"\n",
        "            Calculates the gated-pooling attention score for the image embeddings\n",
        "            :param img_embeddings: (6x768) dimensional\n",
        "            :return single embedding of size (768,)\n",
        "            \"\"\"\n",
        "\n",
        "            # Get weights for gated pooling attention\n",
        "            gpa_weights = torch.softmax(self.w(self.Z(img_embeddings) * self.G(img_embeddings)), dim=0  )\n",
        "\n",
        "            # Take a linear combination of all the image embeddings\n",
        "            fused_embeddings = torch.sum(gpa_weights * img_embeddings, dim=0)\n",
        "\n",
        "            return fused_embeddings\n",
        "\n",
        "        def get_img_embedding(self, imgs):\n",
        "\n",
        "            N = imgs.shape[0]\n",
        "\n",
        "            # Process into patches (N x 6 x 49 x H)\n",
        "            merged_embedding = torch.stack([self.img_model._process_input(img) for img in imgs], dim=0)\n",
        "\n",
        "            # Concatenate the batch class tokens -> (N, 6, 50, H)\n",
        "            batch_class_tokens = self.img_model.class_token.expand(merged_embedding.shape[1], -1, -1).repeat(N, 1, 1, 1)\n",
        "            merged_embedding = torch.cat([batch_class_tokens, merged_embedding], dim=2)\n",
        "\n",
        "            # Add positional embeddings and remove class token -> (N, 6, 49, H)\n",
        "            merged_embedding += self.img_model.encoder.pos_embedding.repeat(N, 1, 1, 1)\n",
        "            merged_embedding = merged_embedding[:, :, 1:]\n",
        "\n",
        "            # Get merged embedding and reshape to 2D embedding -> (N, 1, 49, H)\n",
        "            merged_embedding = torch.stack([self.gpa(embedding.flatten(start_dim=1)).reshape(VIT_SEQ_LENGTH,\n",
        "                                            VIT_HIDDEN_STATE) for embedding in merged_embedding], dim=0)\n",
        "\n",
        "            # Project to VL dimension -> (1, 49, H) (H is 512 for t5-small, 768 for t5-base, 1024 for t5-large)\n",
        "            merged_embedding = self.img_projection_layer(merged_embedding)\n",
        "\n",
        "            # Add modal type embedding to merged embedding\n",
        "            merged_embedding += self.modal_embeddings(\n",
        "                torch.ones((1, merged_embedding.shape[1]), dtype=torch.int, device=device))\n",
        "\n",
        "            return merged_embedding\n",
        "\n",
        "        def forward(self, text_enc, imgs, text_model):\n",
        "\n",
        "            # Get the image embeddings (N x 1 x 49 x H)\n",
        "            imgs_embedding = self.get_img_embedding(imgs)\n",
        "\n",
        "            # Get the text embeddings (N x S x H)\n",
        "            text_embeddings = text_model.get_input_embeddings()(text_enc)\n",
        "\n",
        "            # Add modal embeddings to text\n",
        "            text_embeddings += self.modal_embeddings(torch.zeros((1, text_embeddings.shape[1]), dtype=torch.int,\n",
        "                                                                 device=device))\n",
        "\n",
        "            # Concatenate embeddings -> (1 x S x 512)\n",
        "            merged_embedding = torch.cat([text_embeddings, imgs_embedding], dim=1)\n",
        "\n",
        "            return merged_embedding\n",
        "\n",
        "    def forward(self, text_enc, imgs, labels=None):\n",
        "\n",
        "        # Get the merged embeddings\n",
        "        merged_embedding = self.mvp(text_enc, imgs, self.model)\n",
        "\n",
        "        # If training include the labels\n",
        "        return self.model(inputs_embeds=merged_embedding, labels=labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovGBpXzgvf-r"
      },
      "source": [
        "## Dataset Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlz_XrW7vNzx"
      },
      "outputs": [],
      "source": [
        "class MultiFrameDataset(Dataset):\n",
        "\n",
        "    def __init__(self, input_file, transform=None):\n",
        "        with open(input_file) as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained('google-t5/t5-large')\n",
        "        self.tokenizer.add_tokens('<')\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the question and answer at the idx\n",
        "        qa, img_path = self.data[idx]\n",
        "        img_path = [os.path.join('DriveLM', '/'.join(p.split('\\\\')))\n",
        "                    for p in list(img_path.values())]\n",
        "\n",
        "        q_text, a_text = qa['Q'], qa['A']\n",
        "        q_text = f\"Question: {q_text} Answer:\"\n",
        "\n",
        "        # Concatenate images into a single tensor\n",
        "        imgs = [self.transform(read_image(p).float()).to(device) for p in img_path]\n",
        "        imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "        return q_text, imgs, a_text\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        q_texts, imgs, a_texts = zip(*batch)\n",
        "        imgs = torch.stack(list(imgs), dim=0)\n",
        "\n",
        "        encodings = self.tokenizer(q_texts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "        labels = self.tokenizer(a_texts, padding=True, truncation=True, return_tensors='pt').input_ids.to(device)\n",
        "\n",
        "        return encodings, imgs, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmHYFGAv6Kj"
      },
      "source": [
        "## Downloading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0NwMnpVv5nM"
      },
      "outputs": [],
      "source": [
        "!mkdir DriveLM\n",
        "!mkdir DriveLM/results\n",
        "!unzip -q drive/MyDrive/DriveLM/data.zip -d DriveLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qcfThrJvxNM"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GDJm4u7vy0p"
      },
      "outputs": [],
      "source": [
        "Config = namedtuple('Instance', ['batch_size', 'learning_rate',\n",
        "                                 'weight_decay', 'num_workers',\n",
        "                                 'epochs', 'custom_train', 'gpa_hidden_size',\n",
        "                                 'lora', 'lora_dim', 'lora_alpha', 'lora_dropout',\n",
        "                                 'load_checkpoint', 'file_checkpoint', 'checkpoint_frequency',\n",
        "                                 'freeze_lm', 'lm', 'quantize'])\n",
        "\n",
        "config = Config(\n",
        "    batch_size = 4,\n",
        "    learning_rate = 1e-4,\n",
        "    weight_decay = 0.05,\n",
        "    num_workers = 0,\n",
        "    epochs = 12,\n",
        "    custom_train = True,\n",
        "    gpa_hidden_size = 128,\n",
        "    lora = True,\n",
        "    quantize = True,\n",
        "    lora_dim = 64,\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.05,\n",
        "    load_checkpoint = True,\n",
        "    lm = 'T5',\n",
        "    file_checkpoint = '20240229-205610',\n",
        "    checkpoint_frequency = 10000,\n",
        "    freeze_lm = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lYCS1gjwJAR"
      },
      "source": [
        "## Training Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yrTP9utwJ3D"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def save_model(model, model_name):\n",
        "    # Save the model into the designated folder\n",
        "    path = os.path.join('drive', 'MyDrive', 'DriveLM', 'multi_frame_results', timestr, model_name + '.pth')\n",
        "    torch.save(model, path)\n",
        "\n",
        "\n",
        "def val_model(dloader, val_model):\n",
        "    val_model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    for idx, (inputs, imgs, labels) in tqdm(enumerate(dloader), total=len(dloader)):\n",
        "        outputs = val_model(inputs, imgs, labels)\n",
        "        val_loss += outputs.loss.item()\n",
        "\n",
        "    return val_loss / len(val_dataloader)\n",
        "\n",
        "\n",
        "def save_stats(train_loss, val_loss, epochs, lr):\n",
        "    stats_dict = {\n",
        "        'losses': losses,\n",
        "        'val losses': val_losses,\n",
        "        'min train loss': train_loss,\n",
        "        'min val loss': val_loss,\n",
        "        'epochs': epochs,\n",
        "        'learning rate': lr,\n",
        "        'LM': 'T5-Large',\n",
        "        'Image Embedding': 'Patch'\n",
        "    }\n",
        "\n",
        "    # Save stats into checkpoint\n",
        "    with open(os.path.join('drive', 'MyDrive', 'DriveLM', 'multi_frame_results', timestr, 'stats.json'), 'w') as f:\n",
        "        json.dump(stats_dict, f)\n",
        "\n",
        "\n",
        "def plot_loss(training_loss, val_loss):\n",
        "    num_epochs = len(training_loss)\n",
        "\n",
        "    plt.plot(range(1, num_epochs + 1), training_loss, label='Training Loss')\n",
        "    plt.plot(range(1, num_epochs + 1), val_loss, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Num epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join('drive', 'MyDrive', 'DriveLM', 'multi_frame_results', timestr, 'loss.png'))\n",
        "\n",
        "\n",
        "def custom_train(train_loss, val_loss, best_model, epochs, learning_rate):\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1, verbose=False)\n",
        "\n",
        "    for epoch in range(epochs, config.epochs):\n",
        "        print('-------------------- EPOCH ' + str(epoch) + ' ---------------------')\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for step, (inputs, imgs, labels) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
        "\n",
        "            # Forward pass through model\n",
        "            outputs = model(inputs, imgs, labels)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = outputs.loss\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if step % config.checkpoint_frequency == 0:\n",
        "              print()\n",
        "              print('Loss: ' + str(loss.item()))\n",
        "\n",
        "              # Get the hidden states (output)\n",
        "              hidden_states = outputs.logits\n",
        "\n",
        "              # Perform decoding (e.g., greedy decoding)\n",
        "              outputs = torch.argmax(hidden_states, dim=-1)\n",
        "\n",
        "              text_outputs = [processor.decode(output.to('cpu'), skip_special_tokens=True) for output in outputs]\n",
        "              text_questions = [processor.decode(q.to('cpu'), skip_special_tokens=True) for q in inputs]\n",
        "              text_labels = [processor.decode(a.to('cpu'), skip_special_tokens=True) for a in labels]\n",
        "              print()\n",
        "              print('Questions:')\n",
        "              print(text_questions)\n",
        "              print()\n",
        "              print('Generated Answers:')\n",
        "              print(text_outputs)\n",
        "              print()\n",
        "              print('Ground Truth Answers:')\n",
        "              print(text_labels)\n",
        "\n",
        "            # Back-propogate\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Get train and val loss per batch\n",
        "        epoch_train_loss = epoch_loss / len(train_dataloader)\n",
        "        losses.append(epoch_train_loss)\n",
        "\n",
        "        epoch_val_loss = val_model(val_dataloader, model)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "\n",
        "        if not val_loss or min(epoch_val_loss, val_loss) == epoch_val_loss:\n",
        "            val_loss = epoch_val_loss\n",
        "            best_model = deepcopy(model.state_dict())\n",
        "        if not train_loss or min(train_loss, epoch_train_loss) == epoch_train_loss:\n",
        "            train_loss = epoch_train_loss\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print('Training Loss: ' + str(epoch_train_loss))\n",
        "        print('Validation Loss: ' + str(epoch_val_loss))\n",
        "        print('---------------------------------------------')\n",
        "\n",
        "        # Save model and stats for checkpoints\n",
        "        save_model(best_model, 'latest_model')\n",
        "        epochs += 1\n",
        "        save_stats(train_loss, val_loss, epochs, scheduler.get_last_lr()[0])\n",
        "\n",
        "    # Save the model and plot the loss\n",
        "    plot_loss(losses, val_losses)\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def train():\n",
        "    training_config = TrainingArguments(\n",
        "        output_dir=\"agopalkr/EfficientDriveLM\",\n",
        "        learning_rate=config.learning_rate,\n",
        "        per_device_train_batch_size=config.batch_size,\n",
        "        per_device_eval_batch_size=config.batch_size,\n",
        "        num_train_epochs=config.epochs,\n",
        "        weight_decay=config.weight_decay,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        config=training_config,\n",
        "        train_dataset=train_dset,\n",
        "        eval_dataset=val_dset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    model.push_to_hub(\"agopalkr/EfficientDriveLM\")\n",
        "\n",
        "\n",
        "def save_experiment(statistics):\n",
        "    \"\"\"\n",
        "    Saves the experiment multi_frame_results to a csv\n",
        "    :param config: The hyperparameters used\n",
        "    :param statistics: The accuracies for the training, validation, and test sets\n",
        "    \"\"\"\n",
        "    trial_dict = {\n",
        "        'Model name': [timestr],\n",
        "        'Learning rate': [config.learning_rate],\n",
        "        'Weight decay': [config.weight_decay],\n",
        "        'Batch size': [config.batch_size],\n",
        "        'Epochs': [config.epochs],\n",
        "        'LoRA finetuning': [config.lora],\n",
        "        'GPA Hidden Size': [config.gpa_hidden_size],\n",
        "        'LoRA Dimension': [config.lora_dim],\n",
        "        'LoRA Alpha': [config.lora_alpha],\n",
        "        'LoRA Dropout': [config.lora_dropout],\n",
        "        'Freeze LM': [config.lm],\n",
        "        'Min Training Loss': [statistics[0]],\n",
        "        'Min Validation Loss': [statistics[1]],\n",
        "        'Min Testing Loss': [statistics[2]],\n",
        "    }\n",
        "\n",
        "    trial_dict = pd.DataFrame(trial_dict)\n",
        "    trial_dict.to_csv(os.path.join('drive', 'MyDrive', 'DriveLM', 'multi_frame_results', timestr, 'multi_frame_results.csv'), index=False, header=True)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    min_train_loss = None\n",
        "    min_val_loss = None\n",
        "    best_model = None\n",
        "    epochs_ran = 0\n",
        "\n",
        "    # Load processors and models\n",
        "    model = DriveVLMT5(config)\n",
        "    model.to(device)\n",
        "    print('Trainable Parameters for full model')\n",
        "    print_trainable_parameters(model)\n",
        "    processor = T5Tokenizer.from_pretrained('google-t5/t5-large')\n",
        "    processor.add_tokens('<')\n",
        "\n",
        "    train_dset = MultiFrameDataset(\n",
        "        input_file=os.path.join('DriveLM', 'data', 'multi_frame',\n",
        "                                'multi_frame_train.json'),\n",
        "        transform=transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n",
        "        ])\n",
        "    )\n",
        "    val_dset = MultiFrameDataset(\n",
        "        input_file=os.path.join('DriveLM', 'data', 'multi_frame',\n",
        "                                'multi_frame_val.json'),\n",
        "        transform=transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n",
        "        ])\n",
        "    )\n",
        "    test_dset = MultiFrameDataset(\n",
        "        input_file=os.path.join('DriveLM', 'data', 'multi_frame',\n",
        "                                'multi_frame_test.json'),\n",
        "        transform=transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    # Create Dataloaders\n",
        "    train_dataloader = DataLoader(train_dset, shuffle=True, batch_size=config.batch_size,\n",
        "                                  num_workers=config.num_workers, collate_fn=train_dset.collate_fn)\n",
        "    val_dataloader = DataLoader(val_dset, shuffle=True, batch_size=config.batch_size,\n",
        "                                num_workers=config.num_workers, collate_fn=train_dset.collate_fn)\n",
        "    test_dataloader = DataLoader(test_dset, shuffle=True, batch_size=config.batch_size,\n",
        "                                 num_workers=config.num_workers, collate_fn=train_dset.collate_fn)\n",
        "\n",
        "    if config.custom_train:\n",
        "\n",
        "        # Load checkpoint if neccesary:\n",
        "        if config.load_checkpoint:\n",
        "\n",
        "            print('Loading model from ' + config.file_checkpoint)\n",
        "\n",
        "            # Load the model and stats from the checkpoint\n",
        "            model.load_state_dict(torch.load(os.path.join('drive', 'MyDrive', 'DriveLM', 'multi_frame_results', config.file_checkpoint,\n",
        "                                                          'latest_model.pth')))\n",
        "            best_model = DriveVLMT5(config)\n",
        "            best_model.load_state_dict(torch.load(os.path.join('drive', 'MyDrive', 'DriveLM', 'multi_frame_results', config.file_checkpoint,\n",
        "                                                               'latest_model.pth')))\n",
        "\n",
        "            with open(os.path.join('drive', 'MyDrive', 'DriveLM', 'multi_frame_results', config.file_checkpoint, 'stats.json'), 'r') as f:\n",
        "                stats = json.load(f)\n",
        "\n",
        "            min_train_loss, min_val_loss, losses, val_losses, epochs_ran = stats['min train loss'], \\\n",
        "                                                                           stats['min val loss'], \\\n",
        "                                                                           stats['losses'], stats['val losses'], \\\n",
        "                                                                          stats['epochs']\n",
        "\n",
        "            print(f'Minimum Training Loss: {min_train_loss}')\n",
        "            print(f'Training Losses: {losses}')\n",
        "            print(f'Minimum Validation Loss: {min_val_loss}')\n",
        "            print(f'Validation Losses: {val_losses}')\n",
        "            print(f'Epochs ran: {epochs_ran}')\n",
        "            timestr = config.file_checkpoint\n",
        "        else:\n",
        "            os.mkdir(os.path.join('drive', 'MyDrive', 'DriveLM', 'multi_frame_results', timestr))\n",
        "\n",
        "        if config.load_checkpoint:\n",
        "          lr = stats['learning rate']\n",
        "        else:\n",
        "          lr = config.learning_rate\n",
        "\n",
        "        min_train_loss, min_val_loss = custom_train(min_train_loss, min_val_loss, best_model, epochs_ran, lr)\n",
        "        best_model = DriveVLMT5(config)\n",
        "        best_model.load_state_dict(torch.load(os.path.join('drive', 'MyDrive', 'DriveLM', 'multi_frame_results', timestr, 'latest_model.pth')))\n",
        "        best_model.to(device)\n",
        "        test_loss = val_model(test_dataloader, best_model)\n",
        "        statistics = [min_train_loss, min_val_loss, test_loss]\n",
        "        save_experiment(statistics)\n",
        "    else:\n",
        "        train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zf8_JhK3xue"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyjjFleDIh6V"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMTPPA9sC/iVXMHEbZ2/BJg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}